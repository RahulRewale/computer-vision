{"cells":[{"cell_type":"markdown","metadata":{"id":"s0Gchq_IgBpi"},"source":["# Object Classification Using AlexNet on Imagenette Dataset"]},{"cell_type":"markdown","metadata":{"id":"cEc7NZ49M1Zt"},"source":["### Install and import required libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2929,"status":"ok","timestamp":1651927235577,"user":{"displayName":"Rahul Rewale","userId":"16232024313800457976"},"user_tz":-330},"id":"-3hzlrlEgEdk","outputId":"f446a213-1cc8-4dda-d670-f3352cc6202c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=dd617309d2408f226f5a4636bb5d74365e9b50ea253e9103d91a5283371cb6b2\n","  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n"]}],"source":["# if you want to see GPU memory usage uncomment nvidia-ml-py3\n","!pip install wget # nvidia-ml-py3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"apbkYOn-gJo-"},"outputs":[],"source":["# load all the required packages\n","import os\n","import wget\n","import glob\n","import time\n","import gc\n","#import psutil\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow.keras as K\n","# import nvidia_smi # if you want to see GPU memory usage\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","mpl.rcParams['figure.dpi'] = 300"]},{"cell_type":"markdown","metadata":{"id":"XbPzBHJDNHer"},"source":["### Create variables for storing some paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aW8LJ-2lgLOT"},"outputs":[],"source":["# set dataset related variables\n","data_folder = 'data'\n","dataset_folder = os.path.join(data_folder, 'imagenette2')\n","dataset_tar_file_name = 'imagenette2.tgz'\n","dataset_path = os.path.join(data_folder, dataset_tar_file_name)\n","dataset_url = 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz'\n","classes = ['Tench', 'English Springer', 'Cassette Player', 'Chain Saw', 'Church', \n","\t\t\t'French Horn', 'Garbage Truck', 'Gas Pump', 'Golf Ball', 'Parachute']"]},{"cell_type":"markdown","metadata":{"id":"t2255NimNOkd"},"source":["### Download Imagenette dataset and extract it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XjN70YocgORd"},"outputs":[],"source":["# create a folder for downloading and extracting dataset\n","if not os.path.exists('data'):\n","  os.makedirs('data')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":105663,"status":"ok","timestamp":1651927344073,"user":{"displayName":"Rahul Rewale","userId":"16232024313800457976"},"user_tz":-330},"id":"d_tlVdvCgPyK","outputId":"55ac6e4a-cd91-49a4-b935-9638ba0793df"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-07 12:40:37--  https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.226.240\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.226.240|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1557161267 (1.5G) [application/x-tar]\n","Saving to: ‘data/imagenette2.tgz’\n","\n","data/imagenette2.tg 100%[===================>]   1.45G  16.3MB/s    in 94s     \n","\n","2022-05-07 12:42:12 (15.8 MB/s) - ‘data/imagenette2.tgz’ saved [1557161267/1557161267]\n","\n"]}],"source":["# download and extract dataset\n","!wget -nc {dataset_url}  -O {dataset_path}\n","!tar -xf {dataset_path} -C {data_folder}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jsOe4YDgp0Zz"},"outputs":[],"source":["# if you want to see GPU memory usage\n","# def get_gpu_usage():\n","#   # Reference: https://stackoverflow.com/a/59568642/6764989\n","#   nvidia_smi.nvmlInit()\n","#   handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n","#   # card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate\n","#   info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n","#   print(f\"(Total, Free, Used): ({info.total/2**20}, {info.free/2**20}, {info.used/2**20}) MB\")\n","#   nvidia_smi.nvmlShutdown()"]},{"cell_type":"markdown","metadata":{"id":"8kqyb8_8NZ6Z"},"source":["### Create a class for loading training, validation, testing dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8P093Wtu4JzW"},"outputs":[],"source":["class DataLoader():\n","  \"\"\"Class to load training, validation, and testing data\"\"\"\n","  \n","  def __init__(self, train_dir, val_dir, test_dir=None):\n","    \"\"\" Sets the directories from where data should be loaded\n","\n","    Parameters:\n","    train_dir: directory containing the training data\n","    val_dir: directory containing the validation data\n","    test_dir: directory containing the testing data\n","    \"\"\"\n","\n","    self.train_dir = train_dir\n","    self.val_dir = val_dir\n","    if test_dir:\n","      self.test_dir = test_dir\n","    else:     # we will split the val data into val and test data\n","      self.test_dir = val_dir\n","\n","\n","  def load_train_ds(self, batch_size=64, image_size=(256, 256)):\n","    \"\"\"Loads testing and validation data\n","    \n","    Parameter:\n","    batch_size: batch size for loading data\n","    image_size: target image size that we want for images\n","    \"\"\"\n","\n","    train_ds = K.preprocessing.image_dataset_from_directory(\n","\t\t\t\tdirectory = self.train_dir,\n","\t\t\t\tlabel_mode = 'categorical',\n","\t\t\t\tbatch_size = 4,\n","        # manually set to 4; after generating crops, user specified \n","        # batch_size will be used\n","\t\t\t\timage_size = image_size,\n","\t\t\t\t)\n","\n","    train_ds = train_ds.map(lambda x, y: self.custom_extract_crops(x, y))\n","    train_ds = train_ds.unbatch().shuffle(5000).batch(batch_size)\n","\n","    # print(\"Training dataset size:\", train_ds.cardinality())\n","    # print(\"Sample Training data batch shapes:\")\n","    # for batch_x, batch_y in train_ds.take(2):\n","    #   print(batch_x.shape)\n","    #   print(batch_y.shape)\n","\n","    return train_ds\n","\n","\n","  def load_val_test_ds(self, batch_size=64, image_size=(256, 256), test_split=0.3):\n","    \"\"\"Loads testing and validation data\n","    \n","    Parameter:\n","    batch_size: batch size for loading data\n","    image_size: target image size that we want for images\n","    test_split: if val_dir and test_dir are same, data will be split\n","    \"\"\"\n","\n","    all_ds = K.preprocessing.image_dataset_from_directory(\n","\t\t\t\t\t  directory=self.val_dir,\n","            label_mode='categorical',\n","\t\t\t\t\t  image_size=image_size,\n","            batch_size=batch_size\n","\t\t\t\t  )\n","\n","    if self.test_dir != self.val_dir:\n","      val_ds = all_ds\n","      test_ds = K.preprocessing.image_dataset_from_directory(\n","\t\t\t\t\t      directory=self.test_dir,\n","                label_mode='categorical',\n","\t\t\t\t\t      image_size=image_size,\n","                batch_size=batch_size\n","\t\t\t\t      )\n","    else:\n","      total_size = all_ds.cardinality()\n","      test_ds = all_ds.take(total_size.numpy() * test_split)\n","      val_ds = all_ds.skip(total_size.numpy() * test_split)\n","\n","    val_ds = val_ds.map(lambda x, y: self.extract_five_crops(x, y))\n","    test_ds = test_ds.map(lambda x, y: self.extract_five_crops(x, y))\n","\n","    # print(\"Validation dataset size:\", val_ds.cardinality())\n","    # print(\"Sample Validation data batch shapes:\")\n","    # for batch_x, batch_y in val_ds.take(2):\n","    #   print(batch_x.shape)\n","    #   print(batch_y.shape)\n","\n","    # print(\"Testing dataset size:\", test_ds.cardinality())\n","    # print(\"Sample Testing data batch shapes:\")\n","    # for batch_x, batch_y in test_ds.take(2):\n","    #   print(batch_x.shape)\n","    #   print(batch_y.shape)\n","\n","    return val_ds, test_ds\n","\n","\n","  def custom_extract_crops(self, batch_x, batch_y):\n","    \"\"\" \n","      Creates patches for a batch of images\n","    \n","      Parameters:\n","      batch_x: batch of images\n","      batch_y: batch of labels for the above images\n","    \"\"\"\n","    \n","    patch_size = 227\n","    # stride 3 creates 100 patches from each image\n","    # stride 6 creates 25 patches from each image\n","    # stride 9 creates 16 patches from each image\n","    stride = 9\n","    patches = tf.image.extract_patches(\n","                batch_x,\n","                sizes=[1, patch_size, patch_size, 1], \n","                strides=[1, stride, stride, 1], \n","                rates=[1,1,1,1], \n","                padding='VALID'\n","              )\n","    row_patches = ((256 - patch_size)//stride) + 1\n","    no_patches = row_patches*row_patches\n","    print(\"Number of patches generated from each training image: \", 2*no_patches) # after mirror reflection\n","\n","    patches = tf.reshape(patches,  (-1, patch_size, patch_size, 3))\n","    flipped_patches = tf.image.flip_left_right(patches)\n","    \n","    patches = tf.reshape(patches,  (-1, no_patches, patch_size, patch_size, 3))\n","    flipped_patches = tf.reshape(flipped_patches,  (-1, no_patches, patch_size, patch_size, 3))\n","\n","    final_patches = tf.concat((patches, flipped_patches), axis=1)\n","    final_patches = tf.reshape(final_patches, (-1, patch_size, patch_size, 3))\n","    \n","    labels = tf.repeat(batch_y, repeats=2*no_patches, axis=0)\n","    labels = tf.reshape(labels, (-1, 10))\n","\n","    return final_patches, labels\n","  \n","  \n","  def extract_five_crops(self, batch_x, batch_y):\n","\n","    patch_size = 227\n","    stride = 29\n","    # take 4 patches from corners of the image of size (256, 256)\n","    raw_patches = tf.image.extract_patches(\n","                    batch_x, \n","                    sizes=[1, patch_size, patch_size, 1], \n","                    strides=[1, stride, stride, 1], \n","                    rates=[1,1,1,1], \n","                    padding=\"VALID\"\n","                  )\n","    patches = tf.reshape(raw_patches, (-1, patch_size, patch_size, 3))\n","\n","    flipped_patches = tf.image.flip_left_right(patches)\n","    \n","    patches = tf.reshape(patches, (-1, 4, patch_size, patch_size, 3))\n","    flipped_patches = tf.reshape(flipped_patches, (-1, 4, patch_size, patch_size, 3))\n","    \n","    # take central crop from the image\n","    central_patches = tf.image.resize(tf.image.central_crop(batch_x, 0.88), (patch_size, patch_size))\n","    flipped_central_patches = tf.image.flip_left_right(central_patches)\n","    \n","    #increase dimensions by one to be able to concatenate\n","    central_patches = central_patches[:, tf.newaxis, ...]\n","    flipped_central_patches = flipped_central_patches[:, tf.newaxis, ...]\n","    \n","    # concatenate above to create 10 patches per image\n","    final_patches = tf.concat((patches, central_patches, flipped_patches, flipped_central_patches), axis=1)\n","    \n","    return final_patches, batch_y"]},{"cell_type":"markdown","metadata":{"id":"CbHlD6dgNjdw"},"source":["### Verify datasets are loaded correctly and crops are correct (Ignore)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mux6u7ulARNK"},"outputs":[],"source":["#DEBUGGING DATALOADER\n","#ds_loader = DataLoader(os.path.join(dataset_folder, 'train'), os.path.join(dataset_folder, 'val'))\n","#train_ds = ds_loader.load_train_ds(batch_size=16)\n","#val_ds, test_ds = ds_loader.load_val_test_ds(batch_size=16)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1VIZSc9QAa0S"},"outputs":[],"source":["#DEBUGGING DATALOADER - check if training crops and their labels are correct\n","#for batch_x, batch_y in train_ds.take(1):\n","#  print(batch_x.shape)\n","#  print(batch_y.shape)\n","\n","#  for x, y in zip(batch_x, batch_y):\n","#    plt.imshow(x)\n","#    plt.title(classes[np.argmax(y)])\n","#    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v5oUiWwUMhQ0"},"outputs":[],"source":["#DEBUGGING DATALOADER - check if val/test crops and their labels are correct\n","# below, replace val_ds with test_ds to verify test crops\n","#for batch_x, batch_y in val_ds.take(1):\n","#  print(batch_x.shape)\n","#  print(batch_y.shape)\n","  \n","#  for patches_x, y in zip(batch_x, batch_y):\n","#    for x in patches_x:\n","#      plt.imshow(x)\n","#      plt.title(classes[np.argmax(y)])\n","#      plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0KydwWWYN54f"},"source":["### Define AlexNet Architecture in a class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PwwCFxZ244Q5"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow.keras as K\n","import tensorflow.keras.layers as tfl\n","\n","\n","class AlexNet(K.models.Model):\n","  \"\"\"This class extends Keras Model class and creates an AlexNet model\"\"\"\n","\n","  def __init__(self, dense_units=512, drop=0.5, weight_decay=0.00001, classes=10):\n","    \"\"\"\n","      Creates all the required layers using Keras\n","      \n","      Parameters:\n","      dense_units: no. of units in the dense layers\n","      drop: dropout for the dense layers\n","      weight_decay: weight decay for all conv. layers\n","      classes: no. of categories/classes of objects\n","    \"\"\"\n","\n","    super().__init__()\n","\n","    # RandomBrightness is not available as a layer in current TF version, \n","    # so moved augmentation outside the model\n","    # self.random_contrast = tfl.RandomContrast(factor=0.2)\n","    # self.random_bright = tfl.RandomBrightness(factor=0.2)\n","\n","    self.scale = tfl.Rescaling(scale=1./255)\n","\n","    self.conv1 = tfl.Conv2D(filters=96, kernel_size=(11, 11), strides=(4, 4), \n","          activation='relu', kernel_initializer='he_normal',\n","          kernel_regularizer=K.regularizers.L2(weight_decay))\n","    # Skipped Local Response Normalization layer from AlexNet as it is proven to be not that useful in VGG paper\n","    self.batch_norm1 = tfl.BatchNormalization()\n","    self.pool1 = tfl.MaxPool2D(pool_size=(3, 3), strides=(2, 2))\n","\n","    self.conv2 = tfl.Conv2D(filters=256, kernel_size=(5, 5), padding='same', \n","          activation='relu', kernel_initializer='he_normal',\n","          kernel_regularizer=K.regularizers.L2(weight_decay))\n","    self.batch_norm2 = tfl.BatchNormalization()\n","    # Skipped Local Response Normalization layer\n","    self.pool2 = tfl.MaxPool2D(pool_size=(3, 3), strides=(2, 2))\n","\n","    self.conv3 = tfl.Conv2D(filters=384, kernel_size=(3, 3), padding='same', \n","          activation='relu', kernel_initializer='he_normal',\n","          kernel_regularizer=K.regularizers.L2(weight_decay))\n","    self.batch_norm3 = tfl.BatchNormalization()\n","    self.conv4 = tfl.Conv2D(filters=384, kernel_size=(3, 3), padding='same', \n","          activation='relu', kernel_initializer='he_normal',\n","          kernel_regularizer=K.regularizers.L2(weight_decay))\n","    self.batch_norm4 = tfl.BatchNormalization()\n","    self.conv5 = tfl.Conv2D(filters=256, kernel_size=(3, 3), padding='same', \n","          activation='relu', kernel_initializer='he_normal',\n","          kernel_regularizer=K.regularizers.L2(weight_decay))\n","    self.batch_norm5 = tfl.BatchNormalization()\n","    self.pool3 = tfl.MaxPool2D(pool_size=(3, 3), strides=(2, 2))\n","\n","    # FC layers with dropout\n","    self.flat = tfl.Flatten()\n","    self.dense1 = tfl.Dense(units=dense_units, activation='relu', \n","                            kernel_initializer='he_normal')\n","    self.drop1 = tfl.Dropout(rate=drop)\n","    self.dense2 = tfl.Dense(units=dense_units, activation='relu',\n","                            kernel_initializer='he_normal')\n","    self.drop2 = tfl.Dropout(rate=drop)\n","    \n","    # output layer\n","    self.classifier = tfl.Dense(units=classes, activation='softmax')\n","\n","\n","  def call(self, inputs, training=None):\n","    \"\"\"Processes inputs through the alexnet layers and returns output\"\"\"\n","    \n","    # not able to save model with data augmentation layers in latest TF; \n","    # moved the layer\n","    # out = self.random_contrast(inputs, training=training)\n","    # out = self.random_bright(out, training=training)\n","\n","    out = self.scale(inputs)\n","    out = self.conv1(out)\n","    out = self.batch_norm1(out, training=training)\n","    out = self.pool1(out)\n","    out = self.conv2(out)\n","    out = self.batch_norm2(out, training=training)\n","    out = self.pool2(out)\n","    out = self.conv3(out)\n","    out = self.batch_norm3(out, training=training)\n","    out = self.conv4(out)\n","    out = self.batch_norm4(out, training=training)\n","    out = self.conv5(out)\n","    out = self.batch_norm5(out, training=training)\n","    out = self.pool3(out)\n","    out = self.flat(out)\n","    out = self.dense1(out)\n","    out = self.drop1(out, training=training)\n","    out = self.dense2(out)\n","    out = self.drop2(out, training=training)\n","    return self.classifier(out)"]},{"cell_type":"markdown","metadata":{"id":"-k_fKbYlN8pv"},"source":["### Create some variables required during training, validation, and testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANAYoY1y7NDg"},"outputs":[],"source":["# set variables required for training\n","train_batch_size = 512\n","eval_batch_size = 128\n","image_size = (256, 256)\n","\n","dense_units = 32\n","drop = 0.4\n","weight_decay = 0.005    # 0.0001 was good but overfitting\n","lr = 1e-4   # Adam\n","start_epoch = 0\n","epochs = 25\n","\n","losses = []\n","val_losses = []\n","accuracies = []\n","val_accuracies = []\n","val_losses_lr_update = [] # used in custom callback function for lr update"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"59Dp-F3tUFce"},"outputs":[],"source":["output_dir = os.path.join('results', f'ckpt-units-{dense_units}-drop-{drop}-wd-{weight_decay}')\n","if not os.path.exists(output_dir):\n","  os.makedirs(output_dir)"]},{"cell_type":"markdown","metadata":{"id":"eJ2wkRIsOJz4"},"source":["### Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4945,"status":"ok","timestamp":1651927349011,"user":{"displayName":"Rahul Rewale","userId":"16232024313800457976"},"user_tz":-330},"id":"ECG7PMlf8Tph","outputId":"69546756-3cb9-4475-c98f-1356e28517af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 9469 files belonging to 10 classes.\n","Number of patches generated from each training image:  32\n","Found 3925 files belonging to 10 classes.\n"]}],"source":["# load data\n","ds_loader = DataLoader(os.path.join(dataset_folder, 'train'), os.path.join(dataset_folder, 'val'))\n","train_ds = ds_loader.load_train_ds(batch_size=train_batch_size)\n","val_ds, test_ds = ds_loader.load_val_test_ds(batch_size=eval_batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ggi2oVHE-1tx"},"outputs":[],"source":["#print(train_ds.cardinality())\n","#print(val_ds.cardinality())\n","#print(test_ds.cardinality())"]},{"cell_type":"markdown","metadata":{"id":"o7KXBlzROO9O"},"source":["### Create AlexNet Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTxHAZUU7zDN"},"outputs":[],"source":["# create AlexNet for training\n","alex_net = AlexNet(dense_units=dense_units, drop=drop, weight_decay=weight_decay)"]},{"cell_type":"markdown","metadata":{"id":"N_zAVevH22ar"},"source":["### Load pre-trained model to fine-tune"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iFPXUvEe207g"},"outputs":[],"source":["# load AlexNet for fine-tuning\n","# alex_net2 = K.models.load_model(os.path.join(output_dir, f'lr-0.002100-ep-10'))\n","# start_epoch = 51\n","# epochs = 57   # epochs\n","# optimizer.learning_rate = 0.001470"]},{"cell_type":"markdown","metadata":{"id":"nFbxDJ4aOTJP"},"source":["### Create optimizer, losses, and metrics for training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJfgJT4F71_M"},"outputs":[],"source":["# optimizer\n","# optimizer = K.optimizers.SGD(learning_rate=lr, momentum=0.9)\n","optimizer = K.optimizers.Adam(learning_rate=lr)\n","\n","# loss function\n","loss_obj = K.losses.CategoricalCrossentropy()\n","\n","# training metrics and loss\n","train_mean_loss = K.metrics.Mean()\n","train_cat_acc = K.metrics.CategoricalAccuracy()\n","\n","# validation/testing metrics and loss\n","mean_loss = K.metrics.Mean()\n","cat_acc = K.metrics.CategoricalAccuracy()"]},{"cell_type":"markdown","metadata":{"id":"llSXe9IvOY9W"},"source":["### Training function using GradientTape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCl9eB9x8Elt"},"outputs":[],"source":["@tf.function\n","def train_step(batch_x, batch_y):\n","  \"\"\"Trains model on one batch of data\"\"\"\n","\n","  with tf.GradientTape() as tape:\n","    pred = alex_net(batch_x, training=True)\n","    loss = loss_obj(batch_y, pred)\n","\n","  grads = tape.gradient(loss, alex_net.trainable_variables)\n","  optimizer.apply_gradients(zip(grads, alex_net.trainable_variables))\n","  train_mean_loss(loss)\n","  train_cat_acc.update_state(batch_y, pred)\n","\n","  del pred, loss\n","  gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"MxOov8XyOenh"},"source":["### Function to compute loss and metrics on given dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Us3-T4P8GQq"},"outputs":[],"source":["# TODO - convert this to tf.function\n","def compute_metrics(ds):\n","  \"\"\"Computes loss and accuracy on the given dataset ds\"\"\"\n","\n","  cat_acc.reset_states()\n","  mean_loss.reset_states()\n","\n","  for batch_x, batch_y in iter(ds):\n","    # Note: getting out of memory error when batch size is set to 256\n","    # this is because we use 10 crops per image. So, when we reshape batch_x before\n","    # passing to AlexNet as below, the input shape actually becomes (2560, 227, 277, 3)\n","    # so effective batch size becomes 2560\n","    pred = alex_net(tf.reshape(batch_x, (-1, 227, 227, 3)), training=False)\n","    pred = tf.reshape(pred, (-1, 10, 10))\n","    pred = tf.reduce_mean(pred, axis=1)\n","    loss = loss_obj(batch_y, pred)\n","    mean_loss(loss)\n","    cat_acc.update_state(batch_y, pred)"]},{"cell_type":"markdown","metadata":{"id":"VCS0bpUEOlIu"},"source":["### Custom function to reduce LR on plateau"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGK2NOxqQH0S"},"outputs":[],"source":["def change_LR():\n","  if len(val_losses_lr_update) < 4:\n","    return False\n","\n","  if any(np.array(val_losses_lr_update)[-3:] <= (val_losses_lr_update[-4]-0.05)):\n","    return False\n","  \n","  return True"]},{"cell_type":"markdown","metadata":{"id":"6p1husYnOrgv"},"source":["### Train, validate, and test the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b6mP4gmJ8JOC","outputId":"c401f95c-42d8-4c17-9d60-4575ff20109b","executionInfo":{"status":"ok","timestamp":1651936177205,"user_tz":-330,"elapsed":8828200,"user":{"displayName":"Rahul Rewale","userId":"16232024313800457976"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of dense units: 32\n","Dropout rate: 0.4\n","Weight Decay: 0.005\n","\n","\n","Epoch 1/25:\n","Learning rate: 0.000100\n","\tLoss for 100/0: 2.315612316131592\n","\tLoss for 200/0: 2.2433462142944336\n","\tLoss for 300/0: 2.193598508834839\n","\tLoss for 400/0: 2.145565986633301\n","\tLoss for 500/0: 2.0968549251556396\n","\n","Training Metrics: Loss 2.060122489929199; Accuracy 0.24900992214679718\n","Validation Metrics: Loss 1.648147463798523; Accuracy 0.48755860328674316\n","Time for epoch: 371.7003483772278 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000100-ep-0/assets\n","\n","\n","Epoch 2/25:\n","Learning rate: 0.000100\n","\tLoss for 100/592: 1.818454384803772\n","\tLoss for 200/592: 1.782657265663147\n","\tLoss for 300/592: 1.7389549016952515\n","\tLoss for 400/592: 1.6896017789840698\n","\tLoss for 500/592: 1.647180438041687\n","\n","Training Metrics: Loss 1.6153815984725952; Accuracy 0.43886300921440125\n","Validation Metrics: Loss 1.360113501548767; Accuracy 0.5852866768836975\n","Time for epoch: 351.02467131614685 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000100-ep-1/assets\n","\n","\n","Epoch 3/25:\n","Learning rate: 0.000100\n","\tLoss for 100/592: 1.439510703086853\n","\tLoss for 200/592: 1.415633201599121\n","\tLoss for 300/592: 1.3844746351242065\n","\tLoss for 400/592: 1.3637361526489258\n","\tLoss for 500/592: 1.3371732234954834\n","\n","Training Metrics: Loss 1.31327223777771; Accuracy 0.5464245080947876\n","Validation Metrics: Loss 1.3331043720245361; Accuracy 0.6217093467712402\n","Time for epoch: 350.6984417438507 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000100-ep-2/assets\n","\n","\n","Epoch 4/25:\n","Learning rate: 0.000100\n","\tLoss for 100/592: 1.2095695734024048\n","\tLoss for 200/592: 1.1896145343780518\n","\tLoss for 300/592: 1.1645196676254272\n","\tLoss for 400/592: 1.1474043130874634\n","\tLoss for 500/592: 1.125257968902588\n","\n","Training Metrics: Loss 1.1091388463974; Accuracy 0.6215578317642212\n","Validation Metrics: Loss 1.2670642137527466; Accuracy 0.6739992499351501\n","Time for epoch: 350.21320724487305 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000100-ep-3/assets\n","\n","\n","Epoch 5/25:\n","Learning rate: 0.000100\n","\tLoss for 100/592: 1.056362271308899\n","\tLoss for 200/592: 1.033054232597351\n","\tLoss for 300/592: 1.0135754346847534\n","\tLoss for 400/592: 0.997052013874054\n","\tLoss for 500/592: 0.9734863042831421\n","\n","Training Metrics: Loss 0.955753743648529; Accuracy 0.6792857050895691\n","Validation Metrics: Loss 1.0656232833862305; Accuracy 0.7219617962837219\n","Time for epoch: 351.1211302280426 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000100-ep-4/assets\n","\n","\n","Epoch 6/25:\n","Learning rate: 0.000100\n","\tLoss for 100/592: 0.9066742062568665\n","\tLoss for 200/592: 0.8890100121498108\n","\tLoss for 300/592: 0.8777092695236206\n","\tLoss for 400/592: 0.8577367663383484\n","\tLoss for 500/592: 0.83529132604599\n","\n","Training Metrics: Loss 0.8219852447509766; Accuracy 0.7297661900520325\n","Validation Metrics: Loss 1.0545321702957153; Accuracy 0.7399927973747253\n","Time for epoch: 350.23254776000977 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000100-ep-5/assets\n","\n","\n","Epoch 7/25:\n","Learning rate: 0.000100\n","\tLoss for 100/592: 0.7802881002426147\n","\tLoss for 200/592: 0.7646420001983643\n","\tLoss for 300/592: 0.7491129040718079\n","\tLoss for 400/592: 0.7296828627586365\n","\tLoss for 500/592: 0.7082587480545044\n","\n","Training Metrics: Loss 0.6963214874267578; Accuracy 0.7745043039321899\n","Validation Metrics: Loss 0.9716756343841553; Accuracy 0.7627118825912476\n","Time for epoch: 349.87037777900696 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000100-ep-6/assets\n","\n","\n","Epoch 8/25:\n","Learning rate: 0.000100\n","\tLoss for 100/592: 0.643975019454956\n","\tLoss for 200/592: 0.6262714266777039\n","\tLoss for 300/592: 0.6178166270256042\n","\tLoss for 400/592: 0.6048083305358887\n","\tLoss for 500/592: 0.5919435620307922\n","\n","Training Metrics: Loss 0.5835555791854858; Accuracy 0.8138300180435181\n","Validation Metrics: Loss 1.091738224029541; Accuracy 0.7324197888374329\n","Time for epoch: 350.0679500102997 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000100-ep-7/assets\n","\n","\n","Epoch 9/25:\n","Learning rate: 0.000100\n","\tLoss for 100/592: 0.5458238124847412\n","\tLoss for 200/592: 0.5423838496208191\n","\tLoss for 300/592: 0.5358229279518127\n","\tLoss for 400/592: 0.5244004726409912\n","\tLoss for 500/592: 0.5182130336761475\n","\n","Training Metrics: Loss 0.5144141316413879; Accuracy 0.8336314558982849\n","Validation Metrics: Loss 1.108684778213501; Accuracy 0.7298954129219055\n","Time for epoch: 349.7915196418762 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000100-ep-8/assets\n","\n","\n","Epoch 10/25:\n","Learning rate: 0.000100\n","\tLoss for 100/592: 0.4774819314479828\n","\tLoss for 200/592: 0.46864309906959534\n","\tLoss for 300/592: 0.4689757227897644\n","\tLoss for 400/592: 0.4654822051525116\n","\tLoss for 500/592: 0.46280717849731445\n","\n","Training Metrics: Loss 0.45882663130760193; Accuracy 0.8479313850402832\n","Validation Metrics: Loss 1.034793496131897; Accuracy 0.7692030072212219\n","Time for epoch: 350.1194121837616 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000100-ep-9/assets\n","\n","\n","Epoch 11/25:\n","Learning rate: 0.000050\n","\tLoss for 100/592: 0.43536508083343506\n","\tLoss for 200/592: 0.4317249059677124\n","\tLoss for 300/592: 0.4268970787525177\n","\tLoss for 400/592: 0.42074477672576904\n","\tLoss for 500/592: 0.417136013507843\n","\n","Training Metrics: Loss 0.4128483831882477; Accuracy 0.8622280359268188\n","Validation Metrics: Loss 0.765254557132721; Accuracy 0.8204110860824585\n","Time for epoch: 349.5140244960785 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000050-ep-10/assets\n","\n","\n","Epoch 12/25:\n","Learning rate: 0.000050\n","\tLoss for 100/592: 0.3869420289993286\n","\tLoss for 200/592: 0.3865939676761627\n","\tLoss for 300/592: 0.3869073987007141\n","\tLoss for 400/592: 0.38217002153396606\n","\tLoss for 500/592: 0.37918272614479065\n","\n","Training Metrics: Loss 0.37816399335861206; Accuracy 0.8712179064750671\n","Validation Metrics: Loss 0.6600229144096375; Accuracy 0.8344752788543701\n","Time for epoch: 349.00317549705505 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000050-ep-11/assets\n","\n","\n","Epoch 13/25:\n","Learning rate: 0.000050\n","\tLoss for 100/592: 0.3657631576061249\n","\tLoss for 200/592: 0.36735832691192627\n","\tLoss for 300/592: 0.36576226353645325\n","\tLoss for 400/592: 0.3627473711967468\n","\tLoss for 500/592: 0.3605155944824219\n","\n","Training Metrics: Loss 0.3597067892551422; Accuracy 0.8757953643798828\n","Validation Metrics: Loss 0.7788071036338806; Accuracy 0.8225748538970947\n","Time for epoch: 349.6367907524109 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000050-ep-12/assets\n","\n","\n","Epoch 14/25:\n","Learning rate: 0.000050\n","\tLoss for 100/592: 0.3414280414581299\n","\tLoss for 200/592: 0.34132611751556396\n","\tLoss for 300/592: 0.34356361627578735\n","\tLoss for 400/592: 0.34221264719963074\n","\tLoss for 500/592: 0.3404475748538971\n","\n","Training Metrics: Loss 0.3401452302932739; Accuracy 0.8809008598327637\n","Validation Metrics: Loss 0.6915003657341003; Accuracy 0.8276234865188599\n","Time for epoch: 349.60555768013 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000050-ep-13/assets\n","\n","\n","Epoch 15/25:\n","Learning rate: 0.000050\n","\tLoss for 100/592: 0.32460853457450867\n","\tLoss for 200/592: 0.3296473026275635\n","\tLoss for 300/592: 0.3301587402820587\n","\tLoss for 400/592: 0.32631155848503113\n","\tLoss for 500/592: 0.32347434759140015\n","\n","Training Metrics: Loss 0.32347187399864197; Accuracy 0.8862835168838501\n","Validation Metrics: Loss 0.7501726150512695; Accuracy 0.8222141861915588\n","Time for epoch: 348.8859739303589 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000050-ep-14/assets\n","\n","\n","Epoch 16/25:\n","Learning rate: 0.000025\n","\tLoss for 100/592: 0.3226298987865448\n","\tLoss for 200/592: 0.32259446382522583\n","\tLoss for 300/592: 0.32133978605270386\n","\tLoss for 400/592: 0.3166433572769165\n","\tLoss for 500/592: 0.3151060938835144\n","\n","Training Metrics: Loss 0.31432753801345825; Accuracy 0.8875409364700317\n","Validation Metrics: Loss 0.6430820226669312; Accuracy 0.8510638475418091\n","Time for epoch: 349.13692116737366 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000025-ep-15/assets\n","\n","\n","Epoch 17/25:\n","Learning rate: 0.000025\n","\tLoss for 100/592: 0.3031393885612488\n","\tLoss for 200/592: 0.30708616971969604\n","\tLoss for 300/592: 0.30740711092948914\n","\tLoss for 400/592: 0.30606669187545776\n","\tLoss for 500/592: 0.3047686219215393\n","\n","Training Metrics: Loss 0.3041452169418335; Accuracy 0.8895540833473206\n","Validation Metrics: Loss 0.654276967048645; Accuracy 0.844933271408081\n","Time for epoch: 349.4132561683655 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000025-ep-16/assets\n","\n","\n","Epoch 18/25:\n","Learning rate: 0.000025\n","\tLoss for 100/592: 0.2973574995994568\n","\tLoss for 200/592: 0.30095478892326355\n","\tLoss for 300/592: 0.2982700765132904\n","\tLoss for 400/592: 0.29782211780548096\n","\tLoss for 500/592: 0.2964658737182617\n","\n","Training Metrics: Loss 0.29701924324035645; Accuracy 0.8912702202796936\n","Validation Metrics: Loss 0.7035273909568787; Accuracy 0.8398845791816711\n","Time for epoch: 349.501672744751 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000025-ep-17/assets\n","\n","\n","Epoch 19/25:\n","Learning rate: 0.000025\n","\tLoss for 100/592: 0.28673702478408813\n","\tLoss for 200/592: 0.29110684990882874\n","\tLoss for 300/592: 0.2935377359390259\n","\tLoss for 400/592: 0.29317954182624817\n","\tLoss for 500/592: 0.2929221987724304\n","\n","Training Metrics: Loss 0.2920234203338623; Accuracy 0.8933724761009216\n","Validation Metrics: Loss 0.6301381587982178; Accuracy 0.8589974641799927\n","Time for epoch: 349.063579082489 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000025-ep-18/assets\n","\n","\n","Epoch 20/25:\n","Learning rate: 0.000012\n","\tLoss for 100/592: 0.28848084807395935\n","\tLoss for 200/592: 0.29029929637908936\n","\tLoss for 300/592: 0.2896406948566437\n","\tLoss for 400/592: 0.2881997227668762\n","\tLoss for 500/592: 0.28649893403053284\n","\n","Training Metrics: Loss 0.2864932417869568; Accuracy 0.8940225839614868\n","Validation Metrics: Loss 0.5890660881996155; Accuracy 0.866209864616394\n","Time for epoch: 348.7281334400177 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000012-ep-19/assets\n","\n","\n","Epoch 21/25:\n","Learning rate: 0.000012\n","\tLoss for 100/592: 0.2830001711845398\n","\tLoss for 200/592: 0.2856394350528717\n","\tLoss for 300/592: 0.28418561816215515\n","\tLoss for 400/592: 0.2840486764907837\n","\tLoss for 500/592: 0.28315165638923645\n","\n","Training Metrics: Loss 0.2836928069591522; Accuracy 0.8945374488830566\n","Validation Metrics: Loss 0.6012294292449951; Accuracy 0.8593580722808838\n","Time for epoch: 349.12678623199463 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000012-ep-20/assets\n","\n","\n","Epoch 22/25:\n","Learning rate: 0.000012\n","\tLoss for 100/592: 0.27537262439727783\n","\tLoss for 200/592: 0.28153422474861145\n","\tLoss for 300/592: 0.28010573983192444\n","\tLoss for 400/592: 0.27995651960372925\n","\tLoss for 500/592: 0.2791904807090759\n","\n","Training Metrics: Loss 0.27892693877220154; Accuracy 0.8961017727851868\n","Validation Metrics: Loss 0.6112675070762634; Accuracy 0.8571943640708923\n","Time for epoch: 349.6982433795929 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000012-ep-21/assets\n","\n","\n","Epoch 23/25:\n","Learning rate: 0.000012\n","\tLoss for 100/592: 0.27573922276496887\n","\tLoss for 200/592: 0.2804211378097534\n","\tLoss for 300/592: 0.2789832353591919\n","\tLoss for 400/592: 0.27785250544548035\n","\tLoss for 500/592: 0.27750617265701294\n","\n","Training Metrics: Loss 0.2776201069355011; Accuracy 0.8965373635292053\n","Validation Metrics: Loss 0.617768406867981; Accuracy 0.8597187399864197\n","Time for epoch: 348.99787402153015 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000012-ep-22/assets\n","\n","\n","Epoch 24/25:\n","Learning rate: 0.000006\n","\tLoss for 100/592: 0.27055662870407104\n","\tLoss for 200/592: 0.2733704149723053\n","\tLoss for 300/592: 0.2732945382595062\n","\tLoss for 400/592: 0.2729375958442688\n","\tLoss for 500/592: 0.272973895072937\n","\n","Training Metrics: Loss 0.27321016788482666; Accuracy 0.8979201912879944\n","Validation Metrics: Loss 0.6074389815330505; Accuracy 0.8586368560791016\n","Time for epoch: 349.6908609867096 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000006-ep-23/assets\n","\n","\n","Epoch 25/25:\n","Learning rate: 0.000006\n","\tLoss for 100/592: 0.2704263925552368\n","\tLoss for 200/592: 0.274893194437027\n","\tLoss for 300/592: 0.27330726385116577\n","\tLoss for 400/592: 0.2719871997833252\n","\tLoss for 500/592: 0.27111589908599854\n","\n","Training Metrics: Loss 0.2706175744533539; Accuracy 0.8983756303787231\n","Validation Metrics: Loss 0.5837822556495667; Accuracy 0.8640461564064026\n","Time for epoch: 349.2722430229187 ms\n","INFO:tensorflow:Assets written to: results/ckpt-units-32-drop-0.4-wd-0.005/lr-0.000006-ep-24/assets\n","\n","\n","Testing Metrics: Loss 0.5989049077033997; Accuracy 0.8602430820465088\n"]}],"source":["# train_ds.cardinality() returns invalid value; \n","# thus, set it to 0 initially, and correct it after first epoch\n","no_of_train_batches = 0\n","\n","print(\"Number of dense units:\", dense_units)\n","print(\"Dropout rate:\", drop)\n","print(\"Weight Decay:\", weight_decay)\n","\n","for epoch in range(start_epoch, epochs):\n","  start_time = time.time()\n","\n","  print(f\"\\n\\nEpoch {epoch+1}/{epochs}:\")\n","  print(f\"Learning rate: {optimizer.learning_rate.numpy():.6f}\")\n","  \n","  # shuffle data before each epoch -- program crashes during shuffling\n","  # there is memory leak in tensorflow - https://github.com/tensorflow/tensorflow/issues/31312\n","  # used_mem = psutil.virtual_memory().used\n","  # print(\"used memory: {} Mb\".format(used_mem / 1024 / 1024))\n","  # train_ds = train_ds.unbatch().shuffle(3000).batch(batch_size)\n","  \n","  train_mean_loss.reset_states()\n","  train_cat_acc.reset_states()\n","  \n","  batch_no = 0\n","  for batch_x, batch_y in iter(train_ds):\n","    batch_no += 1\n","    \n","    # data augmentation - not helpful; accuracy descreased slightly\n","    # modified_batch_x = tf.image.random_brightness(batch_x, 0.3)\n","    # modified_batch_x = tf.image.random_contrast(modified_batch_x, 0.5, 2.0)\n","    \n","    train_step(batch_x, batch_y)\n","    if batch_no % 100 == 0:\n","      print(f\"\\tLoss for {batch_no}/{no_of_train_batches}: {train_mean_loss.result()}\")\n","\n","  no_of_train_batches = batch_no  # get correct cardinality of the dataset\n","\n","  print(f\"\\nTraining Metrics: Loss {train_mean_loss.result()}; Accuracy {train_cat_acc.result()}\")\n","  losses.append(train_mean_loss.result().numpy())\n","  accuracies.append(train_cat_acc.result().numpy())\n","\n","  compute_metrics(val_ds)\n","  print(f\"Validation Metrics: Loss {mean_loss.result()}; Accuracy {cat_acc.result()}\")\n","  val_losses.append(mean_loss.result().numpy())\n","  val_losses_lr_update.append(mean_loss.result().numpy())\n","  val_accuracies.append(cat_acc.result().numpy())\n","\n","  print(f\"Time for epoch: {time.time() - start_time} ms\")\n","  alex_net.save(os.path.join(output_dir, f'lr-{optimizer.learning_rate.numpy():.6f}-ep-{epoch}'))\n","\n","  # if change_LR() and optimizer.learning_rate.numpy() > 0.000001: # for sgd\n","  if change_LR() and optimizer.learning_rate.numpy() > 0.000001: # for adam\n","    optimizer.learning_rate = optimizer.learning_rate * 0.5\n","    val_losses_lr_update.clear()\n","  \n","  # garbage collector - doesn't work\n","  # gc.collect()\n","  # K.backend.clear_session()\n","\n","\n","# test on test data\n","compute_metrics(test_ds)\n","print(f\"\\n\\nTesting Metrics: Loss {mean_loss.result()}; Accuracy {cat_acc.result()}\")\n","\n","losses.append(0)\n","accuracies.append(0)\n","val_losses.append(mean_loss.result().numpy())\n","val_accuracies.append(cat_acc.result().numpy())\n","\n","# save model history to a csv file\n","df = pd.DataFrame(np.vstack((losses, val_losses, accuracies, val_accuracies)).T, columns=[\"loss\", \"val_loss\", \"accuracy\", \"val_accuracy\"])\n","df.to_csv(os.path.join(output_dir, f'hist-eps-{epochs}.csv'))"]},{"cell_type":"markdown","metadata":{"id":"neE9uyVAOxn3"},"source":["### Plot some samples from test data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":33437,"status":"ok","timestamp":1651936781326,"user":{"displayName":"Rahul Rewale","userId":"16232024313800457976"},"user_tz":-330},"id":"BLBcaVuIip-K","outputId":"5f768b6e-68ed-431b-f3a4-a421c7ffa97f"},"outputs":[],"source":["# load specific checkpoint to plot testing samples\n","alex_net = K.models.load_model(os.path.join(output_dir, f'lr-0.000012-ep-19'))\n","\n","batch_x, batch_y = next(test_ds.as_numpy_iterator())\n","pred = alex_net(tf.reshape(batch_x, (-1, 227, 227, 3)), training=False)\n","pred = tf.reshape(pred, (-1, 10, 10))\n","pred = tf.reduce_mean(pred, axis=1).numpy()\n","\n","fig = plt.figure(figsize=(10, 10))\n","\n","for i in range(15):\t# plot few images from the batch\n","\tplt.subplot(5, 3, i+1)\n","\tplt.imshow(batch_x[i, 4]/255.0)\t# take the central crop (index 4) for displaying\n","\tplt.title(classes[np.argmax(pred[i])])\n","\tplt.axis('off')\n","\n","plt.tight_layout()\n","# save the figure\n","plt.savefig(os.path.join(output_dir, f'test-eps-{epochs}.png'))"]},{"cell_type":"markdown","metadata":{"id":"Jck92c9HO2rp"},"source":["### Load some specific checkpoint and compute losses and metrics on validation and test data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33376,"status":"ok","timestamp":1651936917514,"user":{"displayName":"Rahul Rewale","userId":"16232024313800457976"},"user_tz":-330},"id":"oF7lRCsdaiYJ","outputId":"85d7243f-ac0d-4f1b-c43e-d2245b93bfdd"},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n","\n","\n","Validation Metrics: Loss 0.5848488211631775; Accuracy 0.8651280403137207\n","\n","\n","Testing Metrics: Loss 0.5925605893135071; Accuracy 0.8602430820465088\n"]}],"source":["new_model = K.models.load_model(os.path.join(output_dir, f'lr-0.000012-ep-19'))\n","\n","def compute_test(ds):\n","  cat_acc.reset_states()\n","  mean_loss.reset_states()\n","  for batch_x, batch_y in iter(ds):\n","    pred = new_model(tf.reshape(batch_x, (-1, 227, 227, 3)), training=False)\n","    pred = tf.reshape(pred, (-1, 10, 10))\n","    pred = tf.reduce_mean(pred, axis=1)\n","    loss = loss_obj(batch_y, pred)\n","    mean_loss(loss)\n","    cat_acc.update_state(batch_y, pred)\n","\n","compute_test(val_ds)\n","print(f\"\\n\\nValidation Metrics: Loss {mean_loss.result()}; Accuracy {cat_acc.result()}\")\n","compute_test(test_ds)\n","print(f\"\\n\\nTesting Metrics: Loss {mean_loss.result()}; Accuracy {cat_acc.result()}\")"]},{"cell_type":"markdown","source":["Epoch 20: <br>\n","Validation Metrics: Loss 0.5848488211631775; Accuracy 0.8651280403137207 <br>\n","Testing Metrics: Loss 0.5925605893135071; Accuracy 0.8602430820465088 <br>\n","\n","Epoch 25: <br>\n","Validation Metrics: Loss 0.6027409434318542; Accuracy 0.8683735728263855 <br>\n","Testing Metrics: Loss 0.5953443050384521; Accuracy 0.8559027910232544 <br>"],"metadata":{"id":"MiBzCzRRFUjD"}},{"cell_type":"markdown","metadata":{"id":"xCJoXne2O-yv"},"source":["### Mount Google Drive and save results of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32893,"status":"ok","timestamp":1651936573765,"user":{"displayName":"Rahul Rewale","userId":"16232024313800457976"},"user_tz":-330},"id":"1JD4S_9bPMql","outputId":"dc8effd4-a013-48be-f1b6-f51e1e77306e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUDTymp3QV9w"},"outputs":[],"source":["!cp -r {output_dir} {os.path.join('drive', 'MyDrive', 'Colab\\ Notebooks', 'Object\\ Classification', 'AlexNet-2022')}"]},{"cell_type":"markdown","metadata":{"id":"RTBSGlaaGYtk"},"source":["### If you want to clear all saved checkpoints"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ioYhjJM9GEdt"},"outputs":[],"source":["#!rm -rf results"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["CbHlD6dgNjdw"],"machine_shape":"hm","name":"AlexNet-Latest-Run1.ipynb","provenance":[{"file_id":"1ELXg-J_-NLKLyPIoBmbHY77bn5ye0Vby","timestamp":1639660347163}],"authorship_tag":"ABX9TyPvmVAshbKcpuUtTQSQR0Nj"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}